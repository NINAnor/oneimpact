---
title: "Fitting models to estimate the ZOI in RSF setup using bagging and penalized regression"
author: "Bernardo Niebuhr"
format: 
  html:
    self-contained: true
    toc: true
    toc-depth: 4 # default is 3
    toc-title: Contents
    toc-location: left
    number-sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro

Here we reanalyze the resource selection function based on reindeer movement data used in Niebuhr et al. 2023 to estimate the ZOI and the cumulative impacts of tourism infrastructure on reindeer habitat selection.

Add a plot of the area and the GPS points.

Add description of the data. GPS points generated with 3h fix rate. 1 used location for each 9 random locations. 115 female reindeer. Annotated with data on land cover (NORUT map), four PCAs representing bio-geo-climatic variation, and the ZOI of the nearest and cumulative ZOI of infrastructure types. The infrastructures considered are: private cabins and public tourism resorts.

Here we show the workflow for preparing the data, fitting, and checking model fits. We do it using three different fitting procedures: Lasso, Adaptive Lasso, and the "Decay Adaptive Lasso" that we propose here.

# Lasso fitting

## Preparing the data and the model

If the development version of the `oneimpact` package is still not installed, it can be done through the following code:

```{r install, eval=FALSE}
# To install the latest version of the package
library(devtools)
devtools::install_github("NINAnor/oneimpact", ref = "lasso")
```

We start by loading the packages and the annotated data, already ready for analysis. For the preparation of biological and environmental/zone of influence data and data annotation workflow, please check Niebuhr et al. 2023.

```{r load_data}
# load packages
library(oneimpact)
library(glmnet)
library(ggplot2)

# aux
library(tictoc)

# load data
data("reindeer_rsf")
# rename it just for convenience
dat <- reindeer_rsf

# explore columns
colnames(dat)
```

The data set "reindeer_rsf" in the `oneimpact` package contains the wild reindeer data used to fit the resource selection functions using the cumulative ZOI approach in Niebuhr et al. 2023. The response variable `use` is a binary variable showing where a given location was used (1) or not (0, a random location within the population area). The GPS used and available positions were annotated with information on land cover (column `NORUTreclass`), bio-geo-climatic PCAs (columns `norway_pca_klima_axis` 1 to 4) and the zone of influence of private cabins and public resorts (columns starting with `private_cabins` and `public_cabins_high`, respectively). Zone of influence variables include both the ZOI of of the nearest feature and the cumulative ZOI, with radii from 100 m to 20 km. For illustration, we only kept ZOI variables with exponential decay shape.

The predictor variables are not standardized - this is done within the fitting process.

### Model specification

We start by defining the structure of the model to be fitted - the `formula`, in R terminology. To do that, we make use of the function `oneimpact::add_zoi_formula()` to make it easier to add the ZOI metrics with multiple radii in the formula.

```{r model_specification}
# formula initial structure
f <- use ~ private_cabins_XXX + public_cabins_high_XXX +
  NORUTreclass + poly(norway_pca_klima_axis1, 2, raw = TRUE) + 
  poly(norway_pca_klima_axis2, 2, raw = TRUE) +
  norway_pca_klima_axis3 + norway_pca_klima_axis4

# add ZOI terms to the formula
zois <- c(100, 250, 500, 1000, 2500, 5000, 10000, 20000)
ff <- add_zoi_formula(f, zoi_radius = zois, pattern = "XXX", 
                      type = c("cumulative_exp_decay", "nearest_exp_decay"),
                      separator = "_", predictor_table = TRUE)

# get formula
f <- ff$formula
# predictor_table for usage later to map ZOI-like type variables
predictor_table_zoi <- ff$predictor_table
```

Contrary to the traditional sub-set model approach, in which only one ZOI predictor is kept in the model at a time and multiple models are fitted and compared, here we keep all the terms in the formula and use a penalized regression approach to both select the variables and fit the model.

```{r formula}
f
```

The `add_zoi_formula()` function can also produce a `predictor_table` `data.frame`, which specifies characteristics of the covariates in the model - e.g. whether they are ZOI metrics or not, which type (cumulative, nearest), which radii. This is helpful to treat the ZOI variables differently in the model interpretation, to aggregate ZOI terms related to the same type of infrastructure, but also to define the term penalties in the "Decay Adaptive Lasso" approach.

```{r predictor_table}
head(predictor_table)
```

### Setting samples

As in other machine learning workflows, as partition the data into sets used to fit (or train) the model, calibrate (or test), and validate. Here this is done for a bootstrap aggregation (bagging) procedure, so not necessarily all data must be used at a time. We use the function `oneimpact::create_resamples()` to this purpose, where we define the number of times we-ll do resampling (parameter `times`) and the proportion of the data observation that goes into fitting, calibration, and validation (parameter `p`). For simplicity, we start performing random sampling.

```{r samples}
# sampling - random sampling
set.seed(1234)
samples <- create_resamples(y = dat$use,
                            p = c(0.2, 0.2, 0.2),
                            times = 50,
                            colH0 = NULL)
```

The object `samples` is a list of three elements: a list of sets (defined by the row numbers in the original dataset) that will be used for model fitting (`samples$train`), for variable selection/calibration (`samples$test`), and for model valiation (`samples$validate`).

```{r samples2}
str(samples)
```

## Fitting the model

To fit one single model (e.g. the one corresponding to the first resample above) using penalized regression, we can use the function `oneimpact::fit_net_logit()` which call `glmnet::glmnet()` for the fitting procedure. We give an example below. By default, a Lasso fit is performed, but the `method` parameter might be used to change it for a Ridge or Adaptive Lasso regression. Notice that observations with missing values in the data resamples need to be removed for fitting, so the actual number of observations used for fitting, calibration, and validation might be actually smaller than it was set. A warning message is printed in these cases.

```{r fit1model}
mod <- fit_net_logit(f, dat, samples, i = 1, metric = proc_AUC)
```

We can just examine teh structure of the output object now. It is a list with the selected lambda parameter (useful in case the model must be re-run), a matrix of estimated coefficients for each of the terms in the formula, the names of the variables included in the `model.matrix`, and scores for fit/train, calibration/test, and validation sets. The score is set by the parameter `metric` in the `fit_net_logit` function, which represents a function used for model .

```{r 1model}
str(mod)
```

Most interestingly, we are here interested in bootstrapping from the whole data set and producing a bag of models. In this case, we can use the function `oneimpact::bag_fit_net_logit()` which fits all the models and produces a list with all the outputs. After fitting, the function `oneimpact::bag_models()` can be used to organize the output of each model in a single "bag" object, of the class `bag`.

```{r bag_of_models, warning=FALSE}
# fit multiple models
tic()
fittedl <- bag_fit_net_logit(f, dat,
                             samples = samples,
                             standardize = "internal", # glmnet does the standardization of covariates
                             metric = proc_AUC, # use pROC::AUC
                             parallel = FALSE)
toc()

# bag models in a single object
bag_summary <- bag_models(fittedl, dat, weights_function = w_strech_max_squared)
```

The resulting bag of models is a list which include the number of models run `n`, the original formula fitted (`formula`), the fitting method (`method`), the validation metric function (`function`), a matrix of coefficients (`coef`) and the validation scores (`validation_score`) for all models.

The function `bag_models()` also transforms the validation scores into weights, so that each model coefficients might be weighted according to how well they fit the data. Models with a validation score below a certain threshold (parameter `score_threshold`) are set to weight zero, while the others are transformed and normalized (to sum 1) according to any standard or user-defined function (set by the parameter `weights_function`). As a consequence, a number of objects related to the weights and the weighted validation scores is also present in the bag object, as well as summaries of the data that are useful for model prediction.

```{r look_bag}
str(bag_summary)
```

**Explain better the default weighing function and the options here**

## Interpreting the model

Once the model was fit, a number of diagnostics and plots can be used to understand the model fit.

### Variable importance

Variable importance is computed here by the function `oneimpact::variable_importance()` by dropping certain terms in the model (parameter `type = "drop"`), recomputing the validation score, and comparing it to the validation score of the full model. The greater the difference in scores, the largest is the importance set to a certain variable or set of variables. This can also be done through permutation of the values of each variable or term (parameter `type = "permutation"`).

Variable importance can then be visualized using the function `oneimpact::plot_importance()`.

```{r variable_importance1}
# variable importance
importance <- variable_importance(bag_summary, dat, type = "drop",
                                  order = "asc")
# importance <- importance[order(-importance)]
plot_importance(importance)
plot_importance(importance, remove_threshold = 5e-3) # remove vars with too low score from plot
plot_importance(importance[order(names(importance))]) # original sequence
```

Variable importance might also be computed for groups of variables. For instance, below we group all variables with similar ZOI metric (cumulative or nearest) and all terms related to the same variable (e.g. quadratic terms).

```{r variable_importance2}
# Using variable block/type of variable
variable_blocks <- bag_summary$var_names |>
  strsplit(split = "_exp_decay|reclass|, 2)|, 2, raw = TRUE)") |>
  sapply(function(x) x[1]) |>
  sub(pattern = "poly(", replacement = "", fixed = TRUE)

importance_block <- variable_importance(bag_summary, dat, type = "drop",
                                        order = "asc",
                                        variable_block = variable_blocks)
plot_importance(importance_block, normalize = T)
```

Variable can be aggregated even more, for instance to evaluate the effect of all terms related to private cabins and public resorts (regardless of the ZOI metric, shape, and radius).

```{r variable_importance3}
# more aggregation of infrastructure, both cumulative and nearest
variable_blocks2 <- bag_summary$var_names |>
  strsplit(split = "_cumulative_exp_decay|_nearest_exp_decay|reclass|, 2)|, 2, raw = TRUE)") |>
  sapply(function(x) x[1]) |>
  sub(pattern = "poly(", replacement = "", fixed = TRUE)

importance_block2 <- variable_importance(bag_summary, dat, type = "drop",
                                         order = "asc",
                                         variable_block = variable_blocks2)
plot_importance(importance_block2)
```

### Model coefficients

The estimated coefficients from the models in the bag can be seen as:

```{r coef1}
# coefficients - already unstandardized by the fit_net_logit function
bag_summary$coef |> 
  head(10)
```

If we want to use the standardized coefficients (for comparison between variables), we can use the function `oneimpact::rescale_coefficients()`.

```{r coef2}
# standardize coefficients again
rescale_coefficients(bag_summary, dat) |> 
  head(10)
```

What is really going to be used for prediction, however, are the weighted coefficients. To understand that, it is important to understand that the model weights in the bag are defined based on the validation scores, and they balance the contribution of the coefficients of each model. Here we see the validation scores and weights:

```{r coef3}
# weights and weighted coefficients
bag_summary$validation_score
bag_summary$weights
```

Now we can get the weighted coefficients for each model, and averaged over models.

```{r coef4}
bag_summary$coef * bag_summary$weights |> # each model
  head()
bag_summary$coef %*% bag_summary$weights # weighted average
```

Finally, we can plot the coefficients in each model in different ways using the `oneimpact::plot_coef()` function.

```{r coef5}
# plot weighted coefficients in each model
# plot_coef(bag_summary) # all models, all terms

# different plots
plot_coef(bag_summary, terms = "private_cabins_cumulative")
plot_coef(bag_summary, terms = "private_cabins_cumulative", 
          plot_type = "histogram")
plot_coef(bag_summary, terms = "private_cabins_cumulative", 
          plot_type = "points")
# in only one or few models
plot_coef(bag_summary, terms = "private_cabins_cumulative", models = 1:3)
```

We can also plot the raw or weighted average coefficients.

```{r coef6}
# plot weighted average coefs
plot_coef(bag_summary, what = "average")
plot_coef(bag_summary, what = "average", terms = "public_cabins", 
          plot_type = "points")
plot_coef(bag_summary, what = "average", terms = "public_cabins", 
          plot_type = "points") + ylim(-50, 50)

# plot raw coefficients, no weighing
# plot_coef(bag_summary, weighted = FALSE, what = "average")
```

### Plot the effect of each predictor in the response variable

We can now plot the response variables one at a time with the `oneimpact::plot_response` function. We fix all variables at their median values (or mean, this is controlled by `baseline` parameter) and vary only one or a few at a time.

#### PCA1

We start by plotting the effect of PCA 1. The green lines below show the average weighted predicted value of the Output (in the y axis), which is proportional to the probability of presence of the species. The blue line represents the weighted median predicted value, and the blue stripe the 95% weighted confidence interval. Just for illustration, we plot the predictions in the linear, exponential, and logistic scale. Below we keep only the logistic representation.

```{r plot_response}
# plot responses

# PCA1
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(norway_pca_klima_axis1 = seq(min(bag_summary$data_summary$norway_pca_klima_axis1),
                                                max(bag_summary$data_summary$norway_pca_klima_axis1),
                                                length.out = 100))
# reference median
plot_response(bag_summary, dfvar, dat, type = "linear", ci = TRUE, 
              wQ_probs = wQ_probs) # linear response
plot_response(bag_summary, dfvar, dat, type = "exponential", ci = TRUE, 
              wQ_probs = wQ_probs) # exponential
plot_response(bag_summary, dfvar, dat, type = "logit", ci = TRUE, 
              wQ_probs = wQ_probs) # logit
```

#### PCA3

Now we plot the effect of PCA3.

```{r plot_response2}
# plot responses

# PCA3
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(norway_pca_klima_axis3 = seq(min(bag_summary$data_summary$norway_pca_klima_axis3),
                                                max(bag_summary$data_summary$norway_pca_klima_axis3),
                                                length.out = 100))
plot_response(bag_summary, dfvar, dat, type = "logit", ci = TRUE, 
              wQ_probs = wQ_probs) # logit
```

#### Private cabins

Now we plot the effects of the ZOI of private cabins. We start by exploring the ZOI of the nearest private cabin. Here the `plot_response()` function gets all the variables that contain "private_cabins_nearest" in the name. We plot the distance in logarithmic scale to ease the visualization.

```{r plot_response3}
# ZOI private cabins nearest
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(private_cabins_nearest = 1e3*seq(0.2, 20, length.out = 100))
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = T, normalize = "mean",
              wQ_probs = wQ_probs, logx = TRUE)
```

We see that... \[interpret here\].

Now we plot the predicted effect of the cumulative ZOI of private cabins. Here the response depends on the number of cabins present in the origin, so we plot the effect for 4 different illustrative values - 1, 10, 100, and 1000 (extreme value) cabins.

```{r plot_response4}
# ZOI private cabins cumulative
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(private_cabins_cumulative = 1e3*seq(0.2, 20, length.out = 100))
# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 10 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 10, plot_mean = FALSE, logx = TRUE)
# 100 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 100, plot_mean = FALSE, logx = TRUE)
# 500
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 500, plot_mean = FALSE, logx = TRUE)
```

Now, what is interesting is the final predicted effect of private cabins, accounting for all the terms related to this type of infrastructure. Agin, we plot this considering different numbers of features. While this does not alter the effect of the terms related to the ZOI of the nearest feature, it does change the effect of the cumulative ZOI terms.

```{r plot_response5}
# ZOI all private cabins nearest and cumulative
wQ_probs=c(0.025, 0.5, 0.975)
# go to account for the number of features for the cumulative variable??
dfvar = data.frame(private_cabins = 1e3*seq(0.2, 20, length.out = 100))

# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 10 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 10, plot_mean = FALSE, logx = TRUE)
# 100 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 100, plot_mean = FALSE, logx = TRUE)
# 150 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 150, plot_mean = FALSE, logx = TRUE)
# 500
plot_response(bag_summary, dfvar, dat, type = "exponential", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 500, plot_mean = FALSE, logx = TRUE)
```

We see that as the number of cabins increase, the cumulative ZOI of private cabins also increases. Different from the statistical approach in Niebuhr et al. 2023, here not only the effect size but the effective radius of the ZOI also changes with the number of features.

#### Public resorts

We start with the ZOI of the nearest resort. We see that, in contrast with private cabins, the effect of the nearest public resort is strong and negative, with a large

```{r plot_response6}
# ZOI public resorts nearest
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(public_cabins_high_nearest = 1e3*seq(0.2, 20, length.out = 100))
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
```

Now we plot the effect of the cumulative ZOI of public resorts:

```{r plot_response7}
# ZOI public resorts cumulative
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(public_cabins_high_cumulative = 1e3*seq(0.2, 20, length.out = 100))
# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 3 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 3, plot_mean = FALSE, logx = TRUE)
```

And the overall effect of public resorts:

```{r plot_response8}
# ZOI all public cabins
wQ_probs=c(0.025, 0.5, 0.975)
# gow to account for the number of features for the cumulative variable??
dfvar = data.frame(public_cabins_high = 1e3*seq(0.2, 20, length.out = 100))
# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 2 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 2, plot_mean = FALSE, logx = TRUE)
```

## Spatial predictions

Implement function.

# Adaptive Lasso fitting

Here we perform the same fitting shown above, but using Adaptive Lasso fitting instead of the pure Lasso. As the preparation of the data and model is the same, we start already by the fitting part.

## Fitting the model

The adaptive Lasso if fitted using the parameter `method = "AdaptiveLass"` in `fit_net_logit` and `bag_fit_net_logit`.

```{r fit1model_al}
mod <- fit_net_logit(f, dat, samples, i = 1, metric = proc_AUC,
                     method = "AdaptiveLasso")
```

We can see that this method squeezes many more coefficients to zero, compared to the Lasso.

```{r 1model_al}
mod$coef
```

Now we fit a bag of models with Adaptive Lasso.

```{r bag_of_models_al, warning=FALSE}
# fit multiple models
tic()
fittedl <- bag_fit_net_logit(f, dat,
                             samples = samples,
                             standardize = "internal", # glmnet does the standardization of covariates
                             metric = proc_AUC, # use pROC::AUC
                             method = "AdaptiveLasso",
                             parallel = FALSE)
toc()

# bag models in a single object
fittedl$models <- fittedl$models[sapply(fittedl$models, class) == "list"]
fittedl$n <- length(fittedl$models)
bag_summary <- bag_models(fittedl, dat, weights_function = w_strech_max_squared)
```

## Interpreting the model

### Variable importance

Overall, the order of importance of the variables is not changed using the Adaptive Lasso instead of the Lasso.

```{r variable_importance1_al}
# variable importance
importance <- variable_importance(bag_summary, dat, type = "drop",
                                  order = "asc")
# importance <- importance[order(-importance)]
plot_importance(importance)
plot_importance(importance, remove_threshold = 5e-3) # remove vars with too low score from plot
plot_importance(importance[order(names(importance))]) # original sequence
```

```{r variable_importance2_al}
# Using variable block/type of variable
variable_blocks <- bag_summary$var_names |>
  strsplit(split = "_exp_decay|reclass|, 2)|, 2, raw = TRUE)") |>
  sapply(function(x) x[1]) |>
  sub(pattern = "poly(", replacement = "", fixed = TRUE)

importance_block <- variable_importance(bag_summary, dat, type = "drop",
                                        order = "asc",
                                        variable_block = variable_blocks)
plot_importance(importance_block, normalize = T)
```

However, in the end by using the Adaptive Lasso the private cabins turn out to be more important:

```{r variable_importance3_al}
# more aggregation of infrastructure, both cumulative and nearest
variable_blocks2 <- bag_summary$var_names |>
  strsplit(split = "_cumulative_exp_decay|_nearest_exp_decay|reclass|, 2)|, 2, raw = TRUE)") |>
  sapply(function(x) x[1]) |>
  sub(pattern = "poly(", replacement = "", fixed = TRUE)

importance_block2 <- variable_importance(bag_summary, dat, type = "drop",
                                         order = "asc",
                                         variable_block = variable_blocks2)
plot_importance(importance_block2)
```

### Model coefficients

```{r coef3_al}
# weights and weighted coefficients
bag_summary$validation_score
bag_summary$weights
```

```{r coef5_al}
# plot weighted coefficients in each model
# plot_coef(bag_summary) # all models, all terms

# different plots
plot_coef(bag_summary, terms = "private_cabins_cumulative")
plot_coef(bag_summary, terms = "private_cabins_cumulative", 
          plot_type = "points")
plot_coef(bag_summary, terms = "private_cabins_cumulative", 
          plot_type = "histogram")
```

We can also plot the raw or weighted average coefficients.

```{r coef6_al}
# plot weighted average coefs
plot_coef(bag_summary, what = "average", terms = "public_cabins", 
          plot_type = "points")
plot_coef(bag_summary, what = "average", terms = "public_cabins", 
          plot_type = "points") + ylim(-50, 50)

plot_coef(bag_summary, what = "average", terms = "private_cabins", 
          plot_type = "points")
```

### Plot the effect of each predictor in the response variable

#### PCA1

```{r plot_response_al}
# plot responses

# PCA1
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(norway_pca_klima_axis1 = seq(min(bag_summary$data_summary$norway_pca_klima_axis1),
                                                max(bag_summary$data_summary$norway_pca_klima_axis1),
                                                length.out = 100))
# reference median
plot_response(bag_summary, dfvar, dat, type = "linear", ci = TRUE, 
              wQ_probs = wQ_probs) # linear response
plot_response(bag_summary, dfvar, dat, type = "exponential", ci = TRUE, 
              wQ_probs = wQ_probs) # exponential
plot_response(bag_summary, dfvar, dat, type = "logit", ci = TRUE, 
              wQ_probs = wQ_probs) # logit
```

#### PCA3

```{r plot_response2_al}
# plot responses

# PCA3
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(norway_pca_klima_axis3 = seq(min(bag_summary$data_summary$norway_pca_klima_axis3),
                                                max(bag_summary$data_summary$norway_pca_klima_axis3),
                                                length.out = 100))
plot_response(bag_summary, dfvar, dat, type = "logit", ci = TRUE, 
              wQ_probs = wQ_probs) # logit
```

#### Private cabins

Now we plot the effects of the ZOI of private cabins. We start by exploring the ZOI of the nearest private cabin. Here the `plot_response()` function gets all the variables that contain "private_cabins_nearest" in the name. We plot the distance in logarithmic scale to ease the visualization.

```{r plot_response3_al}
# ZOI private cabins nearest
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(private_cabins_nearest = 1e3*seq(0.2, 20, length.out = 100))
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = T, normalize = "mean",
              wQ_probs = wQ_probs, logx = TRUE)
```

We see that... \[interpret here\].

Now we plot the predicted effect of the cumulative ZOI of private cabins. Here the response depends on the number of cabins present in the origin, so we plot the effect for 4 different illustrative values - 1, 10, 100, and 1000 (extreme value) cabins.

```{r plot_response4_al}
# ZOI private cabins cumulative
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(private_cabins_cumulative = 1e3*seq(0.5, 20, length.out = 100))
# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 10 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 10, plot_mean = FALSE, logx = TRUE)
# 100 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 100, plot_mean = FALSE, logx = TRUE)
# 500
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 500, plot_mean = FALSE, logx = TRUE)
```

Now, what is interesting is the final predicted effect of private cabins, accounting for all the terms related to this type of infrastructure. Agin, we plot this considering different numbers of features. While this does not alter the effect of the terms related to the ZOI of the nearest feature, it does change the effect of the cumulative ZOI terms.

```{r plot_response5_al}
# ZOI all private cabins nearest and cumulative
wQ_probs=c(0.025, 0.5, 0.975)
# go to account for the number of features for the cumulative variable??
dfvar = data.frame(private_cabins = 1e3*seq(0.5, 20, length.out = 100))

# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 10 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 10, plot_mean = FALSE, logx = TRUE)
# 100 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 100, plot_mean = FALSE, logx = TRUE)
# 150 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 150, plot_mean = FALSE, logx = TRUE)
# 500
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 500, plot_mean = FALSE, logx = TRUE)
```

#### Public resorts

We start with the ZOI of the nearest resort. We see that, in contrast with private cabins, the effect of the nearest public resort is strong and negative, with a large

```{r plot_response6_al}
# ZOI public resorts nearest
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(public_cabins_high_nearest = 1e3*seq(0.5, 20, length.out = 100))
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
```

Now we plot the effect of the cumulative ZOI of public resorts:

```{r plot_response7_al}
# ZOI public resorts cumulative
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(public_cabins_high_cumulative = 1e3*seq(0.2, 20, length.out = 100))
# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 3 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 3, plot_mean = FALSE, logx = TRUE)
```

And the overall effect of public resorts:

```{r plot_response8_al}
# ZOI all public cabins
wQ_probs=c(0.025, 0.5, 0.975)
# gow to account for the number of features for the cumulative variable??
dfvar = data.frame(public_cabins_high = 1e3*seq(0.2, 20, length.out = 100))
# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 2 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 2, plot_mean = FALSE, logx = TRUE)
```

# Decay Adaptive Lasso fitting

Here we perform the same fitting shown above, but using the Decay Adaptive Lasso fitting instead of the pure Lasso. As the preparation of the data and model is the same, we start already by the fitting part.

In the Decay Adaptive Lasso, only the ZOI variables are penalized differently. Besides, they are penalized in an increasing order - with penalty factors increasing with the radius of the the ZOI. The assumption is that as the distance increases, the effect of a given infrastructure is expected to decrease, until it vanishes at some point.

## Fitting the model

The adaptive Lasso if fitted using the parameter `method = "DecayAdaptiveLasso"` in `fit_net_logit` and `bag_fit_net_logit`. An extra parameter is needed here: a `predictor_table` matrix stating whether a each of the terms in the formula is or not a ZOI variable and the radius of the ZOI. We use the variable table `predictor_table` which is also output of the `add_zoi_formula()` function.

```{r fit1model_dal}
mod <- fit_net_logit(f, dat, samples, i = 1, metric = proc_AUC,
                     method = "DecayAdaptiveLasso",
                     predictor_table = predictor_table)
```

We can see that this method squeezes many more coefficients to zero, compared to the Lasso, but less than the AdaptiveLasso.

```{r 1model_dal}
mod$coef
```

Now we fit a bag of models with Decay Adaptive Lasso.

```{r bag_of_models_dal, warning=FALSE}
# fit multiple models
tic()
fittedl <- bag_fit_net_logit(f, dat,
                             samples = samples,
                             standardize = "internal", # glmnet does the standardization of covariates
                             metric = proc_AUC, # use pROC::AUC
                             method = "DecayAdaptiveLasso",
                             predictor_table = predictor_table,
                             parallel = FALSE)
toc()

# bag models in a single object
fittedl$models <- fittedl$models[sapply(fittedl$models, class) == "list"]
fittedl$n <- length(fittedl$models)
bag_summary <- bag_models(fittedl, dat, weights_function = w_strech_max_squared)
```

## Interpreting the model

### Variable importance

Overall, the order of importance of the ZOI variables is not changed using the Decay Adaptive Lasso - mostly the PCAs.

```{r variable_importance1_dal}
# variable importance
importance <- variable_importance(bag_summary, dat, type = "drop",
                                  order = "asc")
# importance <- importance[order(-importance)]
plot_importance(importance)
plot_importance(importance, remove_threshold = 5e-3) # remove vars with too low score from plot
plot_importance(importance[order(names(importance))]) # original sequence
```

```{r variable_importance2_dal}
# Using variable block/type of variable
variable_blocks <- bag_summary$var_names |>
  strsplit(split = "_exp_decay|reclass|, 2)|, 2, raw = TRUE)") |>
  sapply(function(x) x[1]) |>
  sub(pattern = "poly(", replacement = "", fixed = TRUE)

importance_block <- variable_importance(bag_summary, dat, type = "drop",
                                        order = "asc",
                                        variable_block = variable_blocks)
plot_importance(importance_block, normalize = T)
```

In the end by using the Decay Adaptive Lasso the public cabins turn out to be more important:

```{r variable_importance3_dal}
# more aggregation of infrastructure, both cumulative and nearest
variable_blocks2 <- bag_summary$var_names |>
  strsplit(split = "_cumulative_exp_decay|_nearest_exp_decay|reclass|, 2)|, 2, raw = TRUE)") |>
  sapply(function(x) x[1]) |>
  sub(pattern = "poly(", replacement = "", fixed = TRUE)

importance_block2 <- variable_importance(bag_summary, dat, type = "drop",
                                         order = "asc",
                                         variable_block = variable_blocks2)
plot_importance(importance_block2)
```

### Model coefficients

```{r coef3_dal}
# weights and weighted coefficients
bag_summary$validation_score
bag_summary$weights
```

```{r coef5_dal}
# plot weighted coefficients in each model
# plot_coef(bag_summary) # all models, all terms

# different plots
plot_coef(bag_summary, terms = "private_cabins_cumulative")
plot_coef(bag_summary, terms = "private_cabins_cumulative", 
          plot_type = "points")
plot_coef(bag_summary, terms = "private_cabins_cumulative", 
          plot_type = "histogram")
```

We can also plot the raw or weighted average coefficients.

```{r coef6_dal}
# plot weighted average coefs
plot_coef(bag_summary, what = "average", terms = "public_cabins", 
          plot_type = "points")
plot_coef(bag_summary, what = "average", terms = "public_cabins", 
          plot_type = "points") + ylim(-50, 50)

plot_coef(bag_summary, what = "average", terms = "private_cabins", 
          plot_type = "points")
```

### Plot the effect of each predictor in the response variable

#### PCA1

```{r plot_response_dal}
# plot responses

# PCA1
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(norway_pca_klima_axis1 = seq(min(bag_summary$data_summary$norway_pca_klima_axis1),
                                                max(bag_summary$data_summary$norway_pca_klima_axis1),
                                                length.out = 100))
# reference median
plot_response(bag_summary, dfvar, dat, type = "linear", ci = TRUE, 
              wQ_probs = wQ_probs) # linear response
plot_response(bag_summary, dfvar, dat, type = "exponential", ci = TRUE, 
              wQ_probs = wQ_probs) # exponential
plot_response(bag_summary, dfvar, dat, type = "logit", ci = TRUE, 
              wQ_probs = wQ_probs) # logit
```

#### PCA3

```{r plot_response2_dal}
# plot responses

# PCA3
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(norway_pca_klima_axis3 = seq(min(bag_summary$data_summary$norway_pca_klima_axis3),
                                                max(bag_summary$data_summary$norway_pca_klima_axis3),
                                                length.out = 100))
plot_response(bag_summary, dfvar, dat, type = "logit", ci = TRUE, 
              wQ_probs = wQ_probs) # logit
```

#### Private cabins

Now we plot the effects of the ZOI of private cabins. We start by exploring the ZOI of the nearest private cabin. Here the `plot_response()` function gets all the variables that contain "private_cabins_nearest" in the name. We plot the distance in logarithmic scale to ease the visualization.

```{r plot_response3_dal}
# ZOI private cabins nearest
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(private_cabins_nearest = 1e3*seq(0.2, 20, length.out = 100))
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = T, normalize = "mean",
              wQ_probs = wQ_probs, logx = TRUE)
```

We see that... \[interpret here\].

Now we plot the predicted effect of the cumulative ZOI of private cabins. Here the response depends on the number of cabins present in the origin, so we plot the effect for 4 different illustrative values - 1, 10, 100, and 1000 (extreme value) cabins.

```{r plot_response4_dal}
# ZOI private cabins cumulative
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(private_cabins_cumulative = 1e3*seq(0.5, 20, length.out = 100))
# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 10 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 10, plot_mean = FALSE, logx = TRUE)
# 100 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 100, plot_mean = FALSE, logx = TRUE)
# 500
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 500, plot_mean = FALSE, logx = TRUE)
```

Now, what is interesting is the final predicted effect of private cabins, accounting for all the terms related to this type of infrastructure. Agin, we plot this considering different numbers of features. While this does not alter the effect of the terms related to the ZOI of the nearest feature, it does change the effect of the cumulative ZOI terms.

```{r plot_response5_dal}
# ZOI all private cabins nearest and cumulative
wQ_probs=c(0.025, 0.5, 0.975)
# go to account for the number of features for the cumulative variable??
dfvar = data.frame(private_cabins = 1e3*seq(0.5, 20, length.out = 100))

# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 10 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 10, plot_mean = FALSE, logx = TRUE)
# 100 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 100, plot_mean = FALSE, logx = TRUE)
# 150 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 150, plot_mean = FALSE, logx = TRUE)
# 500
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 500, plot_mean = FALSE, logx = TRUE)
```

#### Public resorts

We start with the ZOI of the nearest resort. We see that, in contrast with private cabins, the effect of the nearest public resort is strong and negative, with a large

```{r plot_response6_dal}
# ZOI public resorts nearest
bag_summary$data_summary
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(public_cabins_high_nearest = 1e3*seq(0.5, 20, length.out = 100))
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
```

Now we plot the effect of the cumulative ZOI of public resorts:

```{r plot_response7_dal}
# ZOI public resorts cumulative
wQ_probs=c(0.025, 0.5, 0.975)
dfvar = data.frame(public_cabins_high_cumulative = 1e3*seq(0.2, 20, length.out = 100))
# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 3 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 3, plot_mean = FALSE, logx = TRUE)
```

And the overall effect of public resorts:

```{r plot_response8_dal}
# ZOI all public cabins
wQ_probs=c(0.025, 0.5, 0.975)
# gow to account for the number of features for the cumulative variable??
dfvar = data.frame(public_cabins_high = 1e3*seq(0.2, 20, length.out = 100))
# 1 feature
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 1, plot_mean = FALSE, logx = TRUE)
# 2 features
plot_response(bag_summary, dfvar, dat, type = "logit", zoi = TRUE, ci = TRUE,
              wQ_probs = wQ_probs, n_features = 2, plot_mean = FALSE, logx = TRUE)
```

# Using spatially stratified sampling
